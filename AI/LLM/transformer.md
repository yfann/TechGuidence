
## transformer
+ 文字--(tokenization)-->token--(positional embedding)--->vector---attention-->contextualized token embedding

## attention
+ attention weight
    + 找出相关token


## ref
+ [基于transformers的自然语言处理(NLP)入门](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main)
+ [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
+ [learn-nlp-with-transformers](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A01-%E5%89%8D%E8%A8%80)
+ [为什么我还是无法理解transformer？](https://www.zhihu.com/question/596771388/answer/3491511607)

<!-- video -->
+ [生成式AI導論 2024 transformer](https://www.youtube.com/watch?v=uhNsUCb2fJI&list=PLJV_el3uVTsPz6CTopeRp2L2t4aL_KgiI&index=11)
+ [Hung-yi Lee transformer](https://www.youtube.com/watch?v=ugWDIIOHtPA&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=61)