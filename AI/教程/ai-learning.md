
# LLM Architecture

## tokenizer
+ token
    + 一个单词
    + 单词一部分(subword)
    + 一个字符
    + 符号
    + 短语
+ 每个token都有一个唯一id

## ref
+ [Prompt caching: 10x cheaper LLM tokens, but how?](https://ngrok.com/blog/prompt-caching/)